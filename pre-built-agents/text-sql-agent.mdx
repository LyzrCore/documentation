---
title: Text to SQL Agent
sidebarTitle: Text to SQL Agent
description: 'SQL-based LLM-powered conversational analytics'
---
Welcome to the official documentation for DataAnalyzr – your go-to tool for seamless and comprehensive data analysis. DataAnalyzr is an innovative conversational analytics framework that provides access to LLM-integrated advanced data analysis through a conversational interface to derive actionable insights and intuitive visualizations.

With DataAnalyzr you can streamline the complexity of data analytics into a powerful, intuitive, and conversational interface that lets you command data with ease. Whether you're an experienced data scientist or a business analyst, DataAnalyzr opens up a world of possibilities by transforming raw data into actionable insights and engaging visualizations.

DataAnalyzr supports both pythonic and SQL-based analysis, allowing you to choose the best approach for your data analysis needs.
<Info>For documentation on the pythonic analysis, please refer to the [Data Analysis Agent](/pre-built-agents/data-analysis-agent) documentation.</Info>

DataAnalyzr's SQL analysis type enables users to seamlessly integrate SQL-based analysis into their workflow and harness the versatility and efficiency of SQL for extracting actionable insights from their data. With comprehensive support for SQL-based analysis, DataAnalyzr facilitates a seamless and intuitive user experience, enabling users to unleash the full potential of SQL for driving data-driven decision-making and achieving business objectives.


**DataAnalyzr comes in two distinct editions**

| Open Source Edition (OSE)      | Enterprise Edition (EE)                       |
| ------------------------------ | --------------------------------------------- |
| Free to Use                    | Paid Subscription Required                    |
| Limited Features               | Full Suite of Features                        |
| Handles limited data sources   | Handles a wide range of data sources          |
| Only Handles Simple Analysis   | Handles Complex Analysis                      |
| Only Matplotlib Visualizations | Plotly, Matplotlib and Seaborn Visualizations |
| Only PNG as output format      | PNG, SVG, PDF, HTML and JSON output format    |

## **Key Features**

<CardGroup cols={2}>
    <Card title="Versatile Analysis" icon="square-1">
        Choose from SQL-based analysis, or machine learning-driven analysis, or opt to skip analysis altogether and generate insights from the raw data, depending on your project requirements.
    </Card>
    <Card title="Powerful Integration" icon="square-2">
        Seamlessly integrate DataAnalyzr into your existing workflows and applications, leveraging its APIs and versatile capabilities.
    </Card>
    <Card title="Insightful Outputs" icon="square-3">
        Gain valuable insights, generate queries, and receive actionable recommendations, all powered by advanced language models and external services.
    </Card>
    <Card title="Flexible Configuration" icon="square-4">
        Customize analysis parameters, logging settings, and more to suit your specific use cases and preferences.
    </Card>
</CardGroup>

## Getting started

Data analysis with DataAnalyzr includes three simple steps:
<Steps>
  <Step title="Create an instance">
    Create an instance of the DataAnalyzr class with the desired analysistype and API key.
  </Step>
  <Step title="Load data">
    Load data from files, Redshift, PostgreSQL, or SQLite databases using the `get_data` method.
  </Step>
  <Step title="Ask a question">
    Ask a question using the `ask` method to generate visualizations, insights, recommendations, and tasks.
  </Step>
</Steps>

Let's go over these steps one by one for SQL-based data analysis. The first step is to create a class instance.

```python
from lyzr import DataAnalyzr

analyzr = DataAnalyzr(analysis_type="sql", api_key="openai_key")
```

The `analysis_type` parameter can take three options:

1. `ml` - for analysis with Python code, using Pandas, Scikit-learn and other similar packages.
2. `sql` - for SQL analysis.
3. `skip` - if you want to skip the analysis altogether, and get insights directly from the uploaded data.

<Note>Note that you can also provide an `api_key` parameter. This parameter is optional and given as an alternative to setting the environment variable.</Note>
<Info>For documentation on the pythonic analysis, please refer to the [Data Analysis Agent](/pre-built-agents/data-analysis-agent) documentation.</Info>

For details on all the options available in instantiating the `DataAnalyzr` class, [visit the API reference](#api-reference).


## Loading data

DataAnalyzr provides multiple options for connecting with your data. Whether you are working with data files in CSV, Excel, JSON, etc. formats, or you want to connect to an online database in Redshift, or perhaps you have a local SQLite database, there is an option for you.

The class method used to connect with data is `get_data`. It takes three parameters - `db_type`, `config`, and `vector_store_config` - the values of all of these depend on the format of the input data. Let's look at a couple of examples:

### Loading data from files

Collect all your data files in a dictionary, where their names are keys and paths are values. Then pass this dictionary when calling the `get_data` method.

```python
datafiles = {
    "Name of dataset": path/to/dataset.csv,
    "Name of dataset": path/to/dataset.xls,
    "Name of dataset": path/to/dataset.json,
}
# Call the get_data method
analyzr.get_data(
    db_type = "files",
    config = {
            "datasets": datafiles,
            "db_path": "path/to/local/db",
    },
    vector_store_config = {
            "path": "path/to/local/vector/store",
            "remake": True
    }
)
```

The value of `db_type` tells the system:

- which type of data it will need to explore
- what to expect in the config parameter

In the `vector_store_config`, `remake` parameter is set to `True` to ensure that the vector store is remade every time the DataAnalyzr is initialized. This is useful for testing purposes but should be set to `False` in a production environment. Setting remake to `False` will allow the LLM to learn from previous interactions.

### Loading data from Redshift

As a first step, you will need to collect all the Redshift details.

```python
redshift_config = {
    "host": "",
    "port": 3000, # e.g. 5439
    "database": "",
    "user": "",
    "password": "",
    "schema": ["schema1", "public"],
    "tables": ["table1", "table2"]
}
analyzr.get_data(
    db_type = "redshift",
    config = redshift_config,
    vector_store_config = {
            "path": "path/to/local/vector/store",
            "remake": True
    }
)
```

Once again, note that the value of `db_type` tells the system:

- which type of data it will need to explore
- what to expect in the config parameter

While you may pass only one `database` in `config`, the number of tables and schemas is not limited, they are passed as lists. If no `schema` and `tables` are passed, all the tables from the `public` schema are taken.

### Loading data from PostgreSQL

The implementation for PostgreSQL is very similar to that for Redshift. Start by collecting all the DB details.

```python
postgres_config = {
    "host": "",
    "port": 3001, # e.g. 5439
    "database": "",
    "user": "",
    "password": "",
    "schema": ["schema1", "public"],
    "tables": ["table1", "table2"]
}
analyzr.get_data(
    db_type = "postgres",
    config = postgres_config,
    vector_store_config = {
            "path": "path/to/local/vector/store",
            "remake": True
    }
)
```
<Note>
Note that the value of `db_type` is now `postgres`, while everything else is the same.
</Note>

### Loading data from SQLite

A local SQLite database can also be used for analysis with DataAnalyzr. You only need to pass the path to this database in the `config` parameter.

```python
analyzr.get_data(
    db_type = "sqlite",
    config = {"db_path": "path/to/sqlite.db"},
    vector_store_config = {
				"path": "path/to/local/vector/store",
				"remake": True
		}
)
```

Alternatively, if you have a URL which holds the SQLite database, you can pass this URL as the value of `db_path`.

## Getting results

You can use the `DataAnalyzr` object to perform an analysis on the DataFrame by passing an analysis query to the method `ask`. This function enables you to ask questions directly related to the data at hand, allowing DataAnalyzr to process the inquiry and provide the corresponding visualisation, insights, recommendations, and tasks.

A most simple such implementation looks like this:

```python
result = analyzr.ask("Your question here") # returns dict
```

Here, `result` has keys `visualisation`, `insights`, `recommendations` and `tasks`. 

You can control the outputs received from `ask`:

```python
result = analyzr.ask(
    user_input = "Your question here",
    outputs = ["insights"]
)
```

Here, result still has the keys `"visualisation"`, `“insights”`, `“recommendations”` and `“tasks”` but their values are changed.

```python
>>> print(result["insight"])
This is the insights generated by the LLM analysis...
>>> print(result["recommendations"])
	
>>> repr(result["recommendations"])
"''"
```

You can also specify the context for the analysis and the generation of outputs. For example, you could let the system know that this analysis is for a specific user profile, or that the outputs should be presented in a specific way.

```python
# A humorous example
result = analyzr.ask(
    "How many columns in my dataset?",
    context = {"insights": "Give your response as a pirate."},
    outputs = ["insights"]
)
print(result['insights'])
- Yer dataset be havin' 11 columns in total, matey.
- Thar be 147 rows o' data, with ranks spannin' from 1 to 147.
- The mean rank be 74, which be the midpoint o' the data, arrr.
```

## Getting visualisations
To retrieve plots from your analysis, use the `ask` method and pass `"visualisation"` in your `outputs` parameter. The dictionary returned has a key `"visualisation"` which contains the path to the `png` image. By default, visualization images are saved to `./generated_plots/plot.png`, but this can be controlled using the `plot_path` parameter. Here's an example:
```python
result = analyzr.ask(
    "Your query here",
    outputs = ["visualisation"],
    plot_path = "my_image.png"
)
```
The result then has a `"visualisation"` key which is simply the value of `plot_path`:
```python
>>>> print(result["visualisation"])
my_image.png
```
It is also possible to pass a directory to `plot_path`. The generated image is then saved in the directory as `plot.png`. Additionally, you may pass a context for the image generation.
```python
result = analyzr.ask(
    "Your query here",
    outputs = ["visualisation"],
    context = {"visualisation": "Your visualisation context here"}
    plot_path = "my_directory"
)
print(result["visualisation"])
> my_directory/plot.png
```
You can then view the image using `pillow` or `matplotlib`:
```python
from PIL import Image

image = Image.open(result["visualisation"])
image.show()

# OR ----

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img = mpimg.imread(result["visualisation"])
imgplot = plt.imshow(img)
plt.axis('off')
plt.show()
```
## API Reference

This is the class and function reference of `DataAnalyzr`. Please refer to the [full user guide](#getting-started) for details on usage.

### **`DataAnalyzr` Class**

```python
DataAnalyzr(
    analysis_type: Literal["sql", "ml", "skip"],
    api_key: Optional[str] = None,
    gen_model: Optional[Union[dict, LLM]] = None,
    analysis_model: Optional[Union[dict, LLM]] = None,
    user_input: Optional[str] = None,
    context: Optional[str] = None,
    log_level: Optional[Literal["NOTSET", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]] = "INFO",
    print_log: Optional[bool] = False, log_filename: Optional[str] = "dataanalyzr.log"
)
```

Constructor method for initializing a `DataAnalyzr` object.

#### **Parameters**

<ParamField path="analysis_type" type="Literal['sql', 'ml', 'skip']" required>
    Type of analysis to perform. It can be one of: `sql`, `ml`, or `skip`.
</ParamField>

<ParamField path="api_key" type="string">
    Optional API key used for accessing external services. If not provided, will attempt to retrieve it from the environment variables `API_KEY` or `OPENAI_API_KEY`.
</ParamField>

<ParamField path="gen_model" type="Union[dictionary, LLM]">
    Optional dictionary specifying the language model for generating text. It can also be an instance of the `LyzrLanguageModel` (LLM) class. If not provided, default language mode is used.
</ParamField>

<ParamField path="analysis_model" type="Union[dictionary, LLM]">
    Optional dictionary specifying the analysis model to be used.
</ParamField>

<ParamField path="user_input" type="string">
    Optional user input for analysis.
</ParamField>

<ParamField path="context" type="string">
    Optional context for analysis.
</ParamField>

<ParamField path="log_level" type="Literal['NOTSET', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']">
    Optional logging level. It can be one of the following: `NOTSET`, `DEBUG`, `INFO`, `WARNING`, `ERROR`, or `CRITICAL`. The default is `INFO`.
</ParamField>

<ParamField path="print_log" type="boolean">
    Optional flag indicating whether to print logs to the console. The default is `False`.
</ParamField>

<ParamField path="log_filename" type="string">
    Optional filename for logging. The default is `dataanalyzr.log`.
</ParamField>

#### **Raises**

<ResponseField name="MissingValueError">
  If `api_key` is not provided or found in the environment variables.
</ResponseField>

#### **Returns**

<ResponseField name="DataAnalyzr" type="DataAnalyzr">
  A `DataAnalyzr` object.
</ResponseField>

<Accordion title="Attributes">
- `api_key`: API key used for accessing external services.
- `analysis_type`: Type of analysis.
- `context`: Context for analysis.
- `user_input`: User input for analysis.
- `log_filename`: Filename for logging.
- `database_connector`: Database connector object.
- `df_dict`: Dictionary of Pandas DataFrames.
- `vector_store`: Vector store object.
- `analysis_steps`: Steps performed during analysis.
- `analysis_output`: Output of the analysis.
- `insights_output`: Output of the insights generation.
</Accordion>

#### **Example**

```python
data_analyzer = DataAnalyzr(analysis_type="sql", api_key="your_api_key")
```


### **`get_data` Method**

```python
get_data(
    db_type: Literal["files", "redshift", "postgres", "sqlite"],
    config: dict,
    vector_store_config: dict
)
```

This method retrieves data from various types of databases or files based on the provided configuration. It also creates a vector store for the data. This method must be called before performing any analysis.

#### **Parameters**

<ParamField path="db_type" type="Literal['files', 'redshift', 'postgres', 'sqlite']">
Type of the database or files from which to retrieve data. It can be one of the following:
1. `files`: Retrieve data from files.
2. `redshift`: Retrieve data from Amazon Redshift.
3. `postgres`: Retrieve data from PostgreSQL.
4. `sqlite`: Retrieve data from SQLite.
</ParamField>

<ParamField path="config" type="dictionary">
    A dictionary containing configuration details for connecting to the specified database or files. The required keys depend on the `db_type`.
</ParamField>

<ParamField path="vector_store_config" type="dictionary">
    A dictionary containing configuration details for the vector store.
</ParamField>

#### **Notes**

The required keys in the `config` dictionary depend on the specified `db_type`:
1. If `db_type` is `files`, the required keys in the `config` dictionary are:
    - `datasets`: A dictionary containing the names of the datasets and their corresponding file paths.
    - `files_kwargs`: A dictionary containing keyword arguments for reading the files.
    - `db_path`: The path to the database.
2. If `db_type` is `redshift`, or `postgres`, the required keys in the `config` dictionary include:
    - `host`: The hostname of the database server.
    - `port`: The port number of the database server.
    - `user`: The username for connecting to the database.
    - `password`: The password for connecting to the database.
    - `database`: The name of the database.
    - `schema`: A list of schema names.
    - `tables`: A list of table names.
3. If `db_type` is `sqlite`, the required key in the `config` dictionary:
    - `db_path`: The path to the SQLite database file.

The required keys in the `vector_store_config` dictionary are:
1. `path`: The path to the vector store.
2. `remake`: A boolean value indicating whether to remake the vector store.

Alternatively, the `vector_store_config` can be set to a custom vector store, or it can be set to `None` if the `analysis_type` of the `DataAnalyzr` is `skip`.

#### **Side Effects**

Sets the `database_connector`, `df_dict`, and `vector_store` attributes of the `DataAnalyzr` instance.

#### **Example**

```python
data_analyzer = DataAnalyzr(analysis_type="sql", api_key="your_api_key")
config = {
    "host": "localhost",
    "port": 5432,
    "user": "user",
    "password": "password",
    "database": "my_database",
    "schema": ["public"],
    "tables": ["my_table"]
}
vector_store_config = {
    "path": "./testing/chromadb/testing/",
    "remake": True
}
data_analyzer.get_data(
    db_type="postgres",
    config=config,
    vector_store_config=vector_store_config
)
```


### **`ask` Method**

```python
ask(
    user_input: str = None,
    outputs: list[Literal["visualisation", "insights", "recommendations", "tasks"]] = None,
    plot_path: str = None,
    rerun_analysis: bool = True,
    use_insights: bool = True,
    recs_format: dict = None,
    recs_output_type: Literal["text", "json"] = None,
    counts: dict = None,
    context: dict = None
) -> dict
```
This method orchestrates the analysis process and generates visualization, insights, recommendations, and tasks based on the specified inputs and parameters. It is the recommended method to use for generating visualization, insights, recommendations, and tasks.

#### **Parameters**

<ParamField path="user_input" type="string">
Input provided by the user for analysis. If not provided, the method uses the user input stored in the `DataAnalyzr` instance.
</ParamField>
<ParamField path="outputs" type="list[Literal['visualisation', 'insights', 'recommendations', 'tasks']]">
List of outputs to generate. It can include any combination of `"visualisation"`, `"insights"`, `"recommendations"`, and `"tasks"`. Default is `["visualisation", "insights", "recommendations", "tasks"]`.
</ParamField>
<ParamField path="plot_path" type="string">
Path to the desired location of the visualization generated. The default is `./generated_plots/plot.png`. If a directory is passed, the image is saved in the directory as `plot.png`.
</ParamField>
<ParamField path="rerun_analysis" type="boolean">
Flag indicating whether to rerun the analysis. The default is `True`.
</ParamField>
<ParamField path="use_insights" type="boolean">
Flag indicating whether to use analysis insights. The default is `True`.
</ParamField>
<ParamField path="recs_format" type="dictionary">
Dictionary specifying the format for the recommendations output. The default is `None`.
</ParamField>
<ParamField path="recs_output_type" type="Literal['text', 'json']">
Output type for the recommendations. It can be either `text` or `json`. The default is `None`.
</ParamField>
<ParamField path="counts" type="dictionary">
Dictionary specifying the number of insights, recommendations, and tasks to generate. The default is `None`.
</ParamField>
<ParamField path="context" type="dictionary">
Dictionary specifying the context for analysis, insights, recommendations, and tasks. The default is `None`.
</ParamField>

#### **Returns**

<ResponseField name="response" type="dictionary">
  Returns a dictionary containing the generated insights, recommendations, and tasks. The keys of the dictionary are `insights`, `recommendations`, and `tasks`.
</ResponseField>

#### **Notes**

1. The method coordinates the analysis process and generates visualisation, insights, recommendations, and tasks based on the specified parameters.
2. It allows for customization of outputs, analysis rerun, insights usage, recommendations format, output type, and counts.
3. The generated outputs are returned as a dictionary.

#### **Example**

```python
data_analyzer = DataAnalyzr(analysis_type="sql", api_key="your_api_key")
config = {
    "host": "localhost",
    "port": 5432,
    "user": "user",
    "password": "password",
    "database": "my_database",
    "schema": ["public"],
    "tables": ["my_table"]
}
vector_store_config = {
    "path": "./testing/chromadb/testing/",
    "remake": True
}
data_analyzer.get_data(db_type="postgres", config=config, vector_store_config=vector_store_config)

user_input = "What are the trends in sales?"
outputs = ["visualisation", "insights", "recommendations", "tasks"]
result = data_analyzer.ask(user_input=user_input, outputs=outputs)

# View the visualisation
from PIL import Image
image = Image.open(result["visualisation"])
image.show()
# Print the insights
print(result["insights"])
# Print the recommendations
print(result["recommendations"])
# Print the tasks
print(result["tasks"])
```

### **`analysis` Method**

```python
analysis(
    user_input: str,
    analysis_context: str
) -> Union[list[pd.DataFrame], dict[pd.DataFrame], pd.DataFrame]
```

This method performs data analysis based on the specified analysis type and user input.

#### **Parameters**

<ParamField path="user_input" type="string">
Input provided by the user for analysis.
</ParamField>
<ParamField path="analysis_context" type="string">
Context for analysis.
</ParamField>

#### **Returns**

<ResponseField name="analysis_output" type="pandas.DataFrame">
Returns the analysis output based on the specified analysis type.
</ResponseField>

#### **Notes**

1. If the analysis type is set to `skip`, the method fetches data frames from the database if not already available and returns them as the analysis output.
2. If the analysis type is set to `sql`, it generates a SQL query and performs SQL-based analysis using the provided user input and context.
3. If the analysis type is set to `ml`, it generates analysis steps and performs pandas and machine learning-based analysis using the provided user input and context.

#### **Side Effects**

Sets the `analysis_output` and `analysis_steps` attributes of the `DataAnalyzr` instance.

#### **Example**

```python
data_analyzer = DataAnalyzr(analysis_type="sql", api_key="your_api_key")
config = {
    "host": "localhost",
    "port": 5432,
    "user": "user",
    "password": "password",
    "database": "my_database",
    "schema": ["public"],
    "tables": ["my_table"]
}
vector_store_config = {
    "path": "./testing/chromadb/testing/",
    "remake": True
}
data_analyzer.get_data(db_type="postgres", config=config, vector_store_config=vector_store_config)

user_input = "What are the trends in sales?"
analysis_context = "You are assisting a sales manager who is looking to understand the trends in sales over the past year."
result = data_analyzer.analysis(user_input=user_input, analysis_context=analysis_context)
```

### **`visualisation` Method**

```python
visualisation(
    user_input: str,
    plot_context: str = None,
    plot_path: str = None
) -> str
```

This method generates visualizations based on the user input and plot context. These visualisations are then saved to the `plot_path`. It is recommended that to generate visualisations, the [`ask` method](#ask-method) should be used instead of this method.

#### **Parameters**

<ParamField path="user_input" type="string">
Input provided by the user for visualization.
</ParamField>
<ParamField path="plot_context" type="string">
Context for visualization. If not provided, the default context of the `DataAnalyzr` instance is used.
</ParamField>
<ParamField path="plot_path" type="string">
Path to save the generated plot. If not provided, a default path is used.
</ParamField>

#### **Returns**

<ResponseField name="plot_path" type="string">
Returns the path where the generated visualization is saved.
</ResponseField>

#### **Notes**

If `plot_path` is not provided, the generated plot is saved to `./generated_plots/plot.png`.

#### **Side Effects**

Updates the `visualisation_output` attribute of the `DataAnalyzr` instance.

### **`insights` Method**

```python
insights(
    user_input: str,
    insights_context: Optional[str] = None,
    n_insights: Optional[int] = 3
) -> str
```

This method generates insights based on the user input, analysis context, and specified insights context. It is recommended that to generate insights, the [`ask` method](#ask-method) should be used instead of this method.

**Parameters**:

<ParamField path="user_input" type="string">
input provided by the user. If not provided, the method uses the user input stored in the `DataAnalyzr` instance.
</ParamField>

<ParamField path="insights_context" type="string">
Optional context for insights generation.
</ParamField>

<ParamField path="n_insights" type="integer">
Optional number of insights to generate. The default is 3.
</ParamField>

**Returns**:

<ResponseField name="insights" type="string">
Returns the generated insights as a string.
</ResponseField>

#### **Notes**:

1. The method utilizes a language model to generate insights based on the provided input and context.
2. The generated insights are influenced by the analysis context, user input, and insights context.
3. The generated insights are formatted and returned as a string.

#### **Side Effects**:

Updates the `insights_output` attribute of the `DataAnalyzr` instance.


### **`recommendations` Method**

```python
recommendations(
    user_input: Optional[str] = None,
    use_insights: Optional[bool] = True,
    recs_format: Optional[dict] = None,
    recommendations_context: Optional[str] = None,
    n_recommendations: Optional[int] = 3,
    output_type: Optional[Literal["text", "json"]] = "text"
) -> Union[str, list[dict]]
```

This method generates recommendations based on the user input, analysis insights, and specified recommendations context. It is recommended that to generate recommendations, the [`ask` method](#ask-method) should be used instead of this method.

#### **Parameters**

<ParamField path="use_insights" type="boolean">
Optional flag indicating whether to use analysis insights. The default is `True`.
</ParamField>

<ParamField path="recs_format" type="dictionary">
Optional dictionary specifying the format for the recommendations. The default is `None`. To be used in conjunction with `output_type` set to `json`.
</ParamField>

<ParamField path="recommendations_context" type="string">
Optional context for generating recommendations. If not provided, a default context will be used.
</ParamField>

<ParamField path="n_recommendations" type="integer">
Optional number of recommendations to generate. The default is 3.
</ParamField>

<ParamField path="output_type" type="Literal['text', 'json']">
Optional output type for the recommendations. It can be either `text` or `json`. The default is `text`.
</ParamField>

#### **Returns**

<ResponseField name="recommendations" type="string">
Returns the generated recommendations as a string.
</ResponseField>

#### **Notes**

1. If `use_insights` is `True`, the method incorporates insights into the recommendations.
2. The format of the recommendations output can be customized using the `recs_format` parameter.
    - If the `output_type` is `text`, this parameter is ignored.
    - If `recs_format` is `None`, the default format will be used:
    
    ```json
            [{
                "Recommendation": "string",
                "Basis of the Recommendation": "string",
                "Impact if implemented": "string",
            }]
    ```
    
3. The output type of the recommendations can be either text or JSON.

#### **Side Effects**

Updates the `recommendations_output` attribute of the `DataAnalyzr` instance.


### **`tasks` Method**

```python
tasks(
    user_input: Optional[str] = None,
    tasks_context: Optional[str] = None,
    n_tasks: Optional[int] = 3
) -> str
```

This method generates tasks based on the `user_input`, analysis insights, and recommendations. It is recommended that to generate tasks, the [`ask` method](#ask-method) should be used instead of this method.

#### **Parameters**

<ParamField path="user_input" type="string">
Optional input provided by the user. If not provided, the method uses the user input stored in the `DataAnalyzr` instance.
</ParamField>

<ParamField path="tasks_context" type="string">
Optional context for generating tasks.
</ParamField>

<ParamField path="n_tasks" type="integer">
Optional number of tasks to generate. The default is 3.
</ParamField>

#### **Returns**

<ResponseField name="tasks" type="string">
Returns the generated tasks as a string.
</ResponseField>

#### **Notes**

1. The method incorporates user input, insights, and recommendations into the generated tasks.
2. The number of tasks to generate can be customized using the `n_tasks` parameter.

#### **Side Effects**

Updates the `tasks_output` attribute of the `DataAnalyzr` instance.
